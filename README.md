

## **ğŸ“Œ Overview**

This project implements a **fault-tolerant, distributed key-value store** using:

* **Python + FastAPI**
* **Consistent Hashing**
* **Replication Factor = 3**
* **Write Quorum = 2**
* **Kubernetes (Minikube)**
* **Automatic Rebalancing on Node Failure**
* **Background Heartbeat Monitoring**

The system is composed of:

* **1 Controller Node**
* **4 Worker Nodes**

The controller manages mapping, membership, heartbeats, and failure detection.
Worker nodes store key-value pairs, replicate data, and rebalance on failures.

---

# **ğŸ“Œ Architecture**

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚    CONTROLLER     â”‚
                      â”‚  (FastAPI, K8s)   â”‚
                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                      â”‚ /register         â”‚
                      â”‚ /heartbeat        â”‚
                      â”‚ /mapping          â”‚
                      â”‚ /nodes            â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                 Heartbeats   â”‚
                              â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚               â”‚                              â”‚               â”‚
â–¼               â–¼                              â–¼               â–¼
WORKER1       WORKER2                        WORKER3          WORKER4
(FastAPI)     (FastAPI)                      (FastAPI)        (FastAPI)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Replicas + Rebalancing + Quorum â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# **ğŸ“Œ Key Features**

### âœ” **Consistent Hashing**

* Controller maintains a hash ring with virtual nodes.
* Keys map to a **primary** + **2 replicas**.

### âœ” **Replication (RF = 3)**

* Primary writes locally.
* Sync replicate to replica1.
* Async replicate to replica2.

### âœ” **Write Quorum**

* Minimum 2 acknowledgements required.

### âœ” **Automatic Heartbeats**

* Workers send `/heartbeat` every 1s.
* Controller marks nodes `UP` or `DOWN`.

### âœ” **Failure Detection + Rebalancing**

When a worker dies:

1. Controller detects timeout.
2. Controller marks it DOWN.
3. Controller notifies all active workers.
4. Workers rebalance their keys to new mapping.

### âœ” **Disaster Recovery**

Even if a worker disappears:

* System automatically restores replication factor.

---

# **ğŸ“Œ Microservices**

## **1ï¸âƒ£ Controller**

* `/register`
* `/heartbeat`
* `/mapping`
* `/nodes`
* Failure detection loop
* Notifies workers on `/node_down`

## **2ï¸âƒ£ Worker**

* `/kv` (PUT, GET)
* `/replicate`
* `/status`
* Rebalancing task
* Heartbeat sender
* Auto registration

---

# **ğŸ“Œ Directory Structure**

```
distributed-kv-python/
â”œâ”€â”€ controller/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ controller-deployment.yaml
â”‚   â”œâ”€â”€ controller-service.yaml
â”‚   â”œâ”€â”€ worker1-deployment.yaml
â”‚   â”œâ”€â”€ worker1-service.yaml
â”‚   â”œâ”€â”€ worker2-deployment.yaml
â”‚   â”œâ”€â”€ worker2-service.yaml
â”‚   â”œâ”€â”€ worker3-deployment.yaml
â”‚   â”œâ”€â”€ worker3-service.yaml
â”‚   â”œâ”€â”€ worker4-deployment.yaml
â”‚   â””â”€â”€ worker4-service.yaml
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

# **ğŸ“Œ Run Locally using Docker Compose**

```
docker-compose up --build
```

Controller:

```
curl http://localhost:8000/nodes
curl "http://localhost:8000/mapping?key=user123"
```

Workers:

* worker1 â†’ 8101
* worker2 â†’ 8201
* worker3 â†’ 8301
* worker4 â†’ 8401

---

# **ğŸ“Œ Deploy on Kubernetes (Minikube)**

## **1. Start Minikube**

```
minikube start --driver=docker
eval $(minikube docker-env)
```

## **2. Build Images inside Minikube**

```
docker build -t distributed-kv-controller:latest ./controller
docker build -t distributed-kv-worker:latest ./worker
```

## **3. Apply Kubernetes Files**

```
kubectl apply -f k8s/
kubectl get pods -n kv-system
```

---

# **ğŸ“Œ Demo: Key Mapping + Replication**

### 1. Get mapping

```
curl "http://localhost:8000/mapping?key=user123"
```

### 2. Write to primary

Example (primary = worker3):

```
curl -X PUT "http://localhost:8301/kv" \
 -H "Content-Type: application/json" \
 -d '{"key":"user123","value":"hello"}'
```

### 3. Check replicas

```
curl http://localhost:8201/kv?key=user123
curl http://localhost:8101/kv?key=user123
curl http://localhost:8401/kv?key=user123
```

---

# **ğŸ“Œ Demo: Failure Handling + Rebalancing**

### 1. Scale down worker3

```
kubectl scale deployment worker3 -n kv-system --replicas=0
```

### 2. Controller marks DOWN

```
curl http://localhost:8000/nodes
```

### 3. Check key exists on new mapping nodes

```
curl "http://localhost:8000/mapping?key=user123"
```

Now worker3 will not appear.

Check:

```
curl http://localhost:8101/kv?key=user123
curl http://localhost:8201/kv?key=user123
curl http://localhost:8401/kv?key=user123
```

---

# **ğŸ“Œ Evaluation Points to Mention During Viva**

* Why consistent hashing avoids large remapping
* Why replication factor = 3
* Why quorum=2 (availability over consistency)
* How controller maintains membership
* How heartbeats detect failures
* Why rebalancing is needed
* Why Kubernetes Deployments restart pods by default
* How failure simulation works using scaling
* Why Minikube was used (local production-like environment)

---

# **ğŸ“Œ Git Commit for This Stage**

```
git add README.md DEMO.md
git commit -m "docs: add complete README and demo instructions for distributed KV system"
```

---


